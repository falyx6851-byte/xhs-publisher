# pipeline.py - å‘å¸ƒæµæ°´çº¿æ ¸å¿ƒé€»è¾‘
# ä¸²è”æŠ“å–ã€AIç”Ÿæˆã€æ¸²æŸ“ã€å‘å¸ƒå…¨æµç¨‹

import asyncio
import os
import sys
import json
import re
import shutil
import glob
from datetime import datetime
from playwright.async_api import async_playwright
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import httpx
from bs4 import BeautifulSoup

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from .xhs_core import XHSGenerator
    from .config_manager import ConfigManager
except ImportError:
    # å°è¯•ç›´æ¥å¯¼å…¥ (å…¼å®¹ old style)
    try:
        from xhs_core import XHSGenerator
        from config_manager import ConfigManager
    except ImportError:
        # å°è¯•ä» core å¯¼å…¥ (å…¼å®¹ root execution)
        from core.xhs_core import XHSGenerator
        from core.config_manager import ConfigManager

# ================== å¸¸é‡ ==================
PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
USER_DATA_DIR = os.path.join(PARENT_DIR, "xhs_browser_data")
LOGS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "logs")
ARCHIVES_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "archives")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(LOGS_DIR, exist_ok=True)
os.makedirs(ARCHIVES_DIR, exist_ok=True)


class Logger:
    """æ—¥å¿—ç®¡ç†å™¨"""
    def __init__(self, callback=None):
        self.callback = callback  # GUI å›è°ƒ
        self.logs = []
        self.log_file = os.path.join(LOGS_DIR, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    
    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        full_msg = f"[{timestamp}] {message}"
        self.logs.append(full_msg)
        
        # å†™å…¥æ–‡ä»¶
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(full_msg + "\n")
        
        # GUI å›è°ƒ
        if self.callback:
            self.callback(full_msg)
        else:
            print(full_msg)
    
    async def save_screenshot(self, page, name):
        """ä¿å­˜é”™è¯¯æˆªå›¾"""
        path = os.path.join(LOGS_DIR, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{name}.png")
        try:
            await page.screenshot(path=path)
            self.log(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {path}")
            return path
        except:
            return None


class PublishPipeline:
    """å‘å¸ƒæµæ°´çº¿"""
    
    def __init__(self, config_manager: ConfigManager, logger: Logger = None):
        self.config = config_manager
        self.logger = logger or Logger()
        self.progress_callback = None  # è¿›åº¦å›è°ƒ (0-100)
        
        # è¿è¡Œæ—¶æ•°æ®
        self.scraped_data = None
        self.ai_data = None
        self.image_paths = []
        self.archive_dir = None
        self.image_template = 'breath'  # é»˜è®¤å›¾æ–‡æ¨¡æ¿
    
    def set_progress_callback(self, callback):
        self.progress_callback = callback
    
    def update_progress(self, value):
        if self.progress_callback:
            self.progress_callback(value)
    
    # ================== 1. æŠ“å–æ¨¡å— ==================
    async def scrape(self, url, headless=False):
        """æŠ“å–æ–‡ç« å†…å®¹"""
        self.logger.log(f"ğŸ•·ï¸ æ­£åœ¨æŠ“å–: {url}")
        self.update_progress(10)
        
        async with async_playwright() as p:
            context = await p.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=headless,
                channel="chrome",
                viewport={'width': 1280, 'height': 800},
                args=["--disable-blink-features=AutomationControlled"]
            )
            page = context.pages[0]
            
            try:
                await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                await page.goto(url, timeout=60000)
                await page.wait_for_timeout(3000)
                
                title = await page.title()
                content = await page.evaluate("""() => {
                    document.querySelectorAll('script, style, nav, footer, iframe').forEach(e => e.remove());
                    return document.body.innerText;
                }""")
                
                self.scraped_data = {
                    "title": title,
                    "url": url,
                    "full_text": content[:15000]
                }
                
                self.logger.log(f"âœ… æŠ“å–æˆåŠŸ: {title[:30]}... ({len(content)}å­—)")
                self.update_progress(25)
                return self.scraped_data
                
            except Exception as e:
                self.logger.log(f"âŒ æŠ“å–å¤±è´¥: {e}")
                await self.logger.save_screenshot(page, "scrape_error")
                return None
            finally:
                await context.close()
    
    async def scrape_lightweight(self, url):
        """è½»é‡çº§ HTTP æŠ“å– (ç”¨äºäº‘ç«¯ç¯å¢ƒï¼Œä¸éœ€è¦æµè§ˆå™¨)"""
        self.logger.log(f"ğŸŒ [è½»é‡æ¨¡å¼] æ­£åœ¨æŠ“å–: {url}")
        self.update_progress(10)
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        
        try:
            async with httpx.AsyncClient(timeout=30, follow_redirects=True) as client:
                response = await client.get(url, headers=headers)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):
                    tag.decompose()
                
                # æå–æ ‡é¢˜
                title = soup.title.string if soup.title else "æœªçŸ¥æ ‡é¢˜"
                
                # æå–æ­£æ–‡
                # å¾®ä¿¡å…¬ä¼—å·ç‰¹å®šé€‰æ‹©å™¨
                article = soup.find('div', id='js_content') or soup.find('div', class_='rich_media_content')
                if article:
                    content = article.get_text(separator='\n', strip=True)
                else:
                    content = soup.body.get_text(separator='\n', strip=True) if soup.body else ""
                
                self.scraped_data = {
                    "title": title.strip() if title else "æœªçŸ¥æ ‡é¢˜",
                    "url": url,
                    "full_text": content[:15000]
                }
                
                self.logger.log(f"âœ… [è½»é‡æ¨¡å¼] æŠ“å–æˆåŠŸ: {self.scraped_data['title'][:30]}... ({len(content)}å­—)")
                self.update_progress(25)
                return self.scraped_data
                
        except httpx.HTTPStatusError as e:
            self.logger.log(f"âŒ HTTP é”™è¯¯: {e.response.status_code}")
            return None
        except Exception as e:
            self.logger.log(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return None
    
    # ================== 2. AI ç”Ÿæˆæ¨¡å— ==================
    def generate_content(self, prompt_template):
        """è°ƒç”¨ Gemini ç”Ÿæˆå†…å®¹"""
        if not self.scraped_data:
            self.logger.log("âŒ æ²¡æœ‰æŠ“å–æ•°æ®")
            return None
        
        api_key = self.config.get_current_api_key()
        model_name = self.config.get_current_model()
        
        if not api_key:
            self.logger.log("âŒ æœªè®¾ç½® API Key")
            return None
        
        if not prompt_template or not prompt_template.strip():
            self.logger.log("âŒ æç¤ºè¯æ¨¡æ¿ä¸ºç©ºï¼Œè¯·å…ˆç¼–è¾‘æ¨¡æ¿å†…å®¹")
            return None
        
        self.logger.log(f"ğŸ§  AI æ­£åœ¨æ€è€ƒ (æ¨¡å‹: {model_name})...")
        self.update_progress(35)
        
        genai.configure(api_key=api_key)
        
        user_prompt = prompt_template.format(
            url=self.scraped_data['url'],
            full_text=self.scraped_data['full_text']
        )
        
        safety_settings = {HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE}
        
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(user_prompt, safety_settings=safety_settings)
            
            txt = response.text
            self.logger.log(f"ğŸ“„ AI è¿”å› {len(txt)} å­—ç¬¦")
            
            # å¤šç§æ–¹å¼æå– JSON
            json_str = None
            
            # æ–¹å¼1: ```json ... ```
            if "```json" in txt:
                try:
                    json_str = txt.split("```json")[1].split("```")[0].strip()
                except:
                    pass
            
            # æ–¹å¼2: ``` ... ```
            if not json_str and "```" in txt:
                try:
                    json_str = txt.split("```")[1].split("```")[0].strip()
                except:
                    pass
            
            # æ–¹å¼3: ç›´æ¥æŸ¥æ‰¾ { ... }
            if not json_str:
                match = re.search(r'\{[\s\S]*\}', txt)
                if match:
                    json_str = match.group(0)
            
            # æ–¹å¼4: ç›´æ¥ä½¿ç”¨åŸæ–‡
            if not json_str:
                json_str = txt.strip()
            
            # å°è¯•è§£æ
            self.ai_data = json.loads(json_str)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            if 'cover_title' not in self.ai_data:
                self.logger.log("âš ï¸ ç¼ºå°‘ cover_title å­—æ®µ")
                return None
            if 'content_body' not in self.ai_data:
                self.logger.log("âš ï¸ ç¼ºå°‘ content_body å­—æ®µ")
                return None
            
            self.logger.log(f"âœ… å†…å®¹ç”ŸæˆæˆåŠŸ: {self.ai_data.get('cover_title', 'No Title')[:20]}")
            self.update_progress(50)
            return self.ai_data
            
        except json.JSONDecodeError as e:
            self.logger.log(f"âŒ JSON è§£æå¤±è´¥: {e}")
            self.logger.log(f"ğŸ“ åŸå§‹è¿”å› (å‰500å­—): {txt[:500] if txt else 'N/A'}")
            return None
        except Exception as e:
            self.logger.log(f"âŒ AI ç”Ÿæˆå¤±è´¥: {e}")
            return None

    
    # ================== 3. æ¸²æŸ“æ¨¡å— ==================
    def render_images(self, output_dir=None):
        """æ¸²æŸ“å°çº¢ä¹¦å›¾ç‰‡"""
        if not self.ai_data:
            self.logger.log("âŒ æ²¡æœ‰ AI æ•°æ®")
            return []
        
        self.logger.log("ğŸ¨ æ­£åœ¨æ¸²æŸ“å›¾ç‰‡...")
        self.update_progress(60)
        
        if not output_dir:
            output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "temp_output")
        
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)
        os.makedirs(output_dir)
        
        try:
            template_name = getattr(self, 'image_template', 'breath') or 'breath'
            generator = XHSGenerator(
                template_name=template_name,
                header_text='AI NEWS',
                footer_text='@AI Daily',
                output_dir=output_dir
            )
            
            # æ¸…æ´—æ ‡é¢˜å¹¶æ™ºèƒ½æ¢è¡Œ
            raw_title = self.ai_data['cover_title'].replace("\\n", "\n")
            clean_title = self._remove_emojis(raw_title)
            clean_title = self._smart_wrap_title(clean_title)
            generator.generate_cover(clean_title)
            
            # æ¸…æ´—æ­£æ–‡
            clean_body = self._remove_emojis(self.ai_data['content_body'])
            generator.generate_body(clean_body)
            
            # æ”¶é›†å›¾ç‰‡
            self.image_paths = []
            cover_path = os.path.join(output_dir, "01_cover.png")
            if os.path.exists(cover_path):
                self.image_paths.append(cover_path)
            
            body_files = glob.glob(os.path.join(output_dir, "02_body_*.png"))
            body_files.sort(key=lambda x: int(re.search(r'body_(\d+)', x).group(1)))
            self.image_paths.extend(body_files)
            
            self.logger.log(f"âœ… æ¸²æŸ“å®Œæˆï¼Œå…± {len(self.image_paths)} å¼ å›¾ç‰‡")
            self.update_progress(75)
            return self.image_paths
            
        except Exception as e:
            self.logger.log(f"âŒ æ¸²æŸ“å¤±è´¥: {e}")
            return []
    
    def _remove_emojis(self, text):
        if not text:
            return ""
        return re.sub(r'[\U00010000-\U0010ffff\u2600-\u26ff\u2700-\u27bf\ufe00-\ufe0f\u2300-\u23ff\u200d\u2b50]', '', text).strip()
    
    def _smart_wrap_title(self, title, max_chars_per_line=7, max_lines=3):
        """
        æ™ºèƒ½æ¢è¡Œæ ‡é¢˜ï¼š
        - æ¯è¡Œä¸è¶…è¿‡ max_chars_per_line ä¸ªå­—ç¬¦
        - ä¸æ‹†åˆ†è‹±æ–‡å•è¯
        - æœ€å¤š max_lines è¡Œ
        """
        if not title:
            return title
        
        # å¦‚æœå·²æœ‰æ¢è¡Œä¸”æ ¼å¼åˆç†ï¼Œä¿æŒåŸæ ·
        existing_lines = title.split('\n')
        all_ok = all(len(line.strip()) <= max_chars_per_line + 3 for line in existing_lines if line.strip())
        if len(existing_lines) > 1 and all_ok:
            return title
        
        # åˆå¹¶æ‰€æœ‰æ–‡å­—
        full_text = title.replace('\n', '')
        
        # æ™ºèƒ½åˆ†è¯ï¼šå°†è‹±æ–‡å•è¯ä½œä¸ºæ•´ä½“
        tokens = []
        current_word = ''
        for char in full_text:
            if char.isascii() and char.isalpha():
                current_word += char
            else:
                if current_word:
                    tokens.append(current_word)
                    current_word = ''
                if char.strip():  # éç©ºç™½å­—ç¬¦
                    tokens.append(char)
        if current_word:
            tokens.append(current_word)
        
        # æŒ‰è¡Œåˆ†é…
        lines = []
        current_line = ''
        current_len = 0
        
        for token in tokens:
            token_len = len(token) if not token.isascii() else len(token)
            
            if current_len + token_len <= max_chars_per_line:
                current_line += token
                current_len += token_len
            else:
                if current_line:
                    lines.append(current_line)
                current_line = token
                current_len = token_len
                
                if len(lines) >= max_lines - 1:
                    # æœ€åä¸€è¡Œæ”¾å‰©ä½™æ‰€æœ‰å†…å®¹
                    break
        
        if current_line:
            lines.append(current_line)
        
        # å¦‚æœè¿˜æœ‰å‰©ä½™ tokenï¼Œè¿½åŠ åˆ°æœ€åä¸€è¡Œ
        remaining_idx = sum(len(l.replace(' ', '')) for l in lines)
        remaining = ''.join(tokens)[remaining_idx:]
        if remaining and lines:
            lines[-1] += remaining
        
        return '\n'.join(lines[:max_lines])
    
    # ================== 4. å‘å¸ƒæ¨¡å— ==================
    async def publish(self, headless=False, auto_publish=True):
        """å‘å¸ƒåˆ°å°çº¢ä¹¦"""
        if not self.image_paths:
            self.logger.log("âŒ æ²¡æœ‰å›¾ç‰‡å¯å‘å¸ƒ")
            return False
        
        self.logger.log("ğŸš€ å¯åŠ¨å‘å¸ƒæµç¨‹...")
        self.update_progress(80)
        
        title = self.ai_data.get('caption_title', 'æœªå‘½å')
        content = self.ai_data['content_body'].replace("## ", "").replace("**", "")
        
        async with async_playwright() as p:
            # äº‘ç«¯æ¨¡å¼å¼ºåˆ¶ä½¿ç”¨ headless
            actual_headless = headless and auto_publish
            
            # ä½¿ç”¨æ ‡å‡†æµè§ˆå™¨ä¸Šä¸‹æ–‡ (å…¼å®¹ GitHub Actions)
            browser = await p.chromium.launch(
                headless=actual_headless,
                args=["--disable-blink-features=AutomationControlled", "--disable-notifications"]
            )
            context = await browser.new_context(
                viewport={'width': 1280, 'height': 800}
            )
            
            # --- Cookie æ³¨å…¥é€»è¾‘ (GitHub Actions ä¸“ç”¨) ---
            xhs_cookie_str = self.config.get("xhs_cookie")
            if xhs_cookie_str:
                self.logger.log("ğŸª æ£€æµ‹åˆ° Cookie é…ç½®ï¼Œæ­£åœ¨æ³¨å…¥...")
                # è§£æ Cookie å­—ç¬¦ä¸² (name=value; name2=value2)
                cookies = []
                try:
                    for item in xhs_cookie_str.split(';'):
                        if '=' in item:
                            name, value = item.strip().split('=', 1)
                            cookies.append({
                                "name": name,
                                "value": value,
                                "domain": ".xiaohongshu.com",
                                "path": "/"
                            })
                    await context.add_cookies(cookies)
                    self.logger.log(f"âœ… æˆåŠŸæ³¨å…¥ {len(cookies)} ä¸ª Cookie")
                except Exception as e:
                     self.logger.log(f"âš ï¸ Cookie æ³¨å…¥å¤±è´¥: {e}")
            # -------------------------------------------

            page = await context.new_page()
            
            try:
                # è®¿é—®å‘å¸ƒé¡µ
                self.logger.log("ğŸŒ å‰å¾€å°çº¢ä¹¦åˆ›ä½œä¸­å¿ƒ...")
                await page.goto("https://creator.xiaohongshu.com/publish/publish", timeout=60000)
                
                # ç™»å½•æ£€æµ‹
                try:
                    await page.wait_for_selector(".creator-container", timeout=15000)
                except:
                    self.logger.log("âš ï¸ éœ€è¦ç™»å½•ï¼Œè¯·æ‰«ç ...")
                    await self.logger.save_screenshot(page, "need_login")
                    await page.wait_for_url("**/publish/publish**", timeout=120000)
                
                await page.wait_for_timeout(3000)
                await page.keyboard.press("Escape")
                
                # åˆ‡æ¢å›¾æ–‡æ¨¡å¼
                self.logger.log("ğŸ”„ åˆ‡æ¢å›¾æ–‡æ¨¡å¼...")
                await page.evaluate("""() => {
                    const allElements = document.querySelectorAll('*');
                    for (const el of allElements) {
                        if (el.innerText && el.innerText.trim() === 'ä¸Šä¼ å›¾æ–‡' && el.offsetParent !== null) {
                            el.click();
                            if(el.parentElement) el.parentElement.click();
                        }
                    }
                }""")
                await page.wait_for_timeout(2000)
                
                # ä¸Šä¼ å›¾ç‰‡
                self.logger.log(f"ğŸ“¤ ä¸Šä¼  {len(self.image_paths)} å¼ å›¾ç‰‡...")
                try:
                    async with page.expect_file_chooser(timeout=10000) as fc_info:
                        buttons = await page.locator("div, button, span").filter(has_text="ä¸Šä¼ å›¾ç‰‡").all()
                        for btn in buttons:
                            if await btn.is_visible():
                                await btn.click()
                                break
                    file_chooser = await fc_info.value
                    await file_chooser.set_files(self.image_paths)
                    self.logger.log("âœ… å›¾ç‰‡ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.log(f"âš ï¸ å›¾ç‰‡ä¸Šä¼ å¼‚å¸¸: {e}")
                
                await page.wait_for_timeout(8000)
                
                # å¡«å†™æ ‡é¢˜
                self.logger.log("âœï¸ å¡«å†™æ ‡é¢˜...")
                try:
                    await page.locator("input[placeholder*='æ ‡é¢˜']").first.fill(title)
                except:
                    await page.locator("input.el-input__inner").first.fill(title)
                
                # å¡«å†™æ­£æ–‡
                self.logger.log("âœï¸ å¡«å†™æ­£æ–‡...")
                editor = page.locator(".ProseMirror")
                if await editor.count() > 0:
                    await editor.click()
                    await page.keyboard.type(content, delay=30)
                
                # è‡ªåŠ¨é€‰æ‹©æ¨èæ ‡ç­¾
                self.logger.log("ğŸ·ï¸ è‡ªåŠ¨é€‰æ‹©æ ‡ç­¾...")
                await page.wait_for_timeout(2000)
                try:
                    for i in range(5):  # ç‚¹å‡»5ä¸ªæ ‡ç­¾
                        # å®šä½ç¬¬ä¸€ä¸ªå¯è§çš„ # å¼€å¤´çš„æ ‡ç­¾
                        tag_selector = ".tag-group > span.tag"
                        tags = page.locator(tag_selector)
                        count = await tags.count()
                        
                        if count > 0:
                            first_tag = tags.first
                            if await first_tag.is_visible():
                                text = await first_tag.inner_text()
                                if text.startswith('#') and 'å±•å¼€' not in text:
                                    self.logger.log(f"   ğŸ‘‰ é€‰æ‹©: {text}")
                                    await first_tag.click()
                                    await page.wait_for_timeout(1500)  # ç­‰å¾…åˆ—è¡¨åˆ·æ–°
                except Exception as e:
                    self.logger.log(f"âš ï¸ æ ‡ç­¾é€‰æ‹©å¼‚å¸¸: {e}")
                
                self.update_progress(90)
                
                # å‘å¸ƒæˆ–æ‰‹åŠ¨
                if auto_publish:
                    self.logger.log("ğŸš€ ç‚¹å‡»å‘å¸ƒ...")
                    await page.wait_for_timeout(3000)
                    
                    # å°è¯•å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾å‘å¸ƒæŒ‰é’®
                    btn = None
                    selectors = [
                        "button.publishBtn",
                        "button:has-text('å‘å¸ƒ')",
                        ".publish-btn",
                        "button.css-1gl8z4q",  # å¤‡ç”¨ç±»å
                        "[class*='publish']"
                    ]
                    
                    for sel in selectors:
                        try:
                            candidate = page.locator(sel).first
                            if await candidate.count() > 0 and await candidate.is_visible(timeout=3000):
                                btn = candidate
                                self.logger.log(f"âœ… æ‰¾åˆ°å‘å¸ƒæŒ‰é’®: {sel}")
                                break
                        except:
                            continue
                    
                    if btn is None:
                        self.logger.log("âŒ æ‰¾ä¸åˆ°å‘å¸ƒæŒ‰é’®")
                        await self.logger.save_screenshot(page, "no_publish_btn")
                        return False
                    
                    try:
                        await btn.click(timeout=10000)
                        self.logger.log("âœ… å‘å¸ƒæŒ‡ä»¤å·²å‘é€ï¼")
                        await page.wait_for_timeout(5000)
                        self.update_progress(100)
                        return True
                    except Exception as e:
                        self.logger.log(f"âŒ ç‚¹å‡»å‘å¸ƒæŒ‰é’®å¤±è´¥: {e}")
                        await self.logger.save_screenshot(page, "publish_click_error")
                        return False
                else:
                    self.logger.log("â¸ï¸ æ‰‹åŠ¨å‘å¸ƒæ¨¡å¼ï¼šè¯·æ£€æŸ¥å†…å®¹åæ‰‹åŠ¨ç‚¹å‡»å‘å¸ƒ")
                    self.update_progress(95)
                    # ç­‰å¾…ç”¨æˆ·æ“ä½œ
                    await page.wait_for_timeout(600000)  # 10åˆ†é’Ÿ
                    return True
                    
            except Exception as e:
                self.logger.log(f"âŒ å‘å¸ƒå‡ºé”™: {e}")
                await self.logger.save_screenshot(page, "publish_error")
                return False
            finally:
                if auto_publish:
                    await page.wait_for_timeout(5000)
                    await context.close()
                    await browser.close()
    
    # ================== 5. å½’æ¡£æ¨¡å— ==================
    def archive(self):
        """å½’æ¡£æ‰€æœ‰å†…å®¹"""
        if not self.scraped_data or not self.ai_data:
            return None
        
        self.logger.log("ğŸ“¦ æ­£åœ¨å½’æ¡£...")
        
        # åˆ›å»ºå½’æ¡£ç›®å½•
        date_str = datetime.now().strftime("%Y-%m-%d")
        title = self.ai_data.get('caption_title', 'æœªå‘½å')[:20]
        safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
        self.archive_dir = os.path.join(ARCHIVES_DIR, f"{date_str}_{safe_title}")
        
        if os.path.exists(self.archive_dir):
            shutil.rmtree(self.archive_dir)
        os.makedirs(self.archive_dir)
        
        # ä¿å­˜åŸæ–‡
        with open(os.path.join(self.archive_dir, "åŸæ–‡.txt"), 'w', encoding='utf-8') as f:
            f.write(f"URL: {self.scraped_data['url']}\n")
            f.write(f"æ ‡é¢˜: {self.scraped_data['title']}\n\n")
            f.write(self.scraped_data['full_text'])
        
        # ä¿å­˜ AI ç”Ÿæˆç»“æœ
        with open(os.path.join(self.archive_dir, "AIç”Ÿæˆ.json"), 'w', encoding='utf-8') as f:
            json.dump(self.ai_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é…ç½®
        config_snapshot = {
            "api_key": self.config.get_current_api_key()[:10] + "...",
            "model": self.config.get_current_model(),
            "silent_mode": self.config.is_silent_mode(),
            "auto_publish": self.config.is_auto_publish()
        }
        with open(os.path.join(self.archive_dir, "é…ç½®.json"), 'w', encoding='utf-8') as f:
            json.dump(config_snapshot, f, ensure_ascii=False, indent=2)
        
        # å¤åˆ¶å›¾ç‰‡
        images_dir = os.path.join(self.archive_dir, "images")
        os.makedirs(images_dir)
        for img_path in self.image_paths:
            if os.path.exists(img_path):
                shutil.copy(img_path, images_dir)
        
        self.logger.log(f"âœ… å½’æ¡£å®Œæˆ: {self.archive_dir}")
        return self.archive_dir
    
    # ================== å®Œæ•´æµç¨‹ ==================
    async def run_full_pipeline(self, url, prompt_template, cloud_mode=False):
        """æ‰§è¡Œå®Œæ•´å‘å¸ƒæµç¨‹
        
        Args:
            url: æ–‡ç« é“¾æ¥
            prompt_template: AI æç¤ºè¯æ¨¡æ¿
            cloud_mode: æ˜¯å¦ä¸ºäº‘ç«¯æ¨¡å¼ (GitHub Actions)ï¼Œäº‘ç«¯æ¨¡å¼ä½¿ç”¨è½»é‡æŠ“å– + Cookieå‘å¸ƒ
        """
        self.update_progress(0)
        
        # 1. æŠ“å– (äº‘ç«¯ç”¨è½»é‡æ¨¡å¼)
        if cloud_mode:
            self.logger.log("â˜ï¸ äº‘ç«¯æ¨¡å¼ï¼šä½¿ç”¨è½»é‡çº§ HTTP æŠ“å–")
            result = await self.scrape_lightweight(url)
        else:
            headless = self.config.is_silent_mode()
            result = await self.scrape(url, headless=headless)
        
        if not result:
            return False
        
        # 2. AI ç”Ÿæˆ
        result = self.generate_content(prompt_template)
        if not result:
            return False
        
        # 3. æ¸²æŸ“
        result = self.render_images()
        if not result:
            return False
        
        # 4. å‘å¸ƒ (äº‘ç«¯æ¨¡å¼ä¹Ÿå‘å¸ƒï¼Œä½¿ç”¨ Cookie è®¤è¯)
        if cloud_mode:
            self.logger.log("â˜ï¸ äº‘ç«¯æ¨¡å¼ï¼šä½¿ç”¨ Cookie è®¤è¯å‘å¸ƒ")
            # äº‘ç«¯å¼ºåˆ¶ headless + è‡ªåŠ¨å‘å¸ƒ
            success = await self.publish(headless=True, auto_publish=True)
        else:
            headless = self.config.is_silent_mode()
            auto_publish = self.config.is_auto_publish()
            success = await self.publish(headless=headless, auto_publish=auto_publish)
        
        # 5. å½’æ¡£
        self.archive()
        
        return success
